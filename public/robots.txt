# robots.txt for https://ahmed-abd-elmohsen.vercel.app
# Last updated: September 12, 2025
#
# This file works together with Next.js app/robots.ts to provide
# instructions to search engines and web crawlers.
# Next.js serves the configured robots.txt from app/robots.ts,
# but this static file serves as a fallback.

# Allow all web crawlers access to all content by default
User-agent: *
Allow: /
Allow: /resume
Allow: /projects/
Allow: /skills
Allow: /contact
Allow: /blog

# Disallow access to private, admin, and other non-public areas
Disallow: /api/
Disallow: /private/
Disallow: /admin/
Disallow: /drafts/
Disallow: /*.json$
Disallow: /*.xml$
Disallow: /analytics/
Disallow: /internal/

# Crawl-delay directive (in seconds) - helps prevent server overload
# Note: This is respected by Bing, Yandex, but not by Google
Crawl-delay: 2

# Specific rules for Google's main crawler
User-agent: Googlebot
Allow: /
Disallow: /api/
Disallow: /private/
Disallow: /admin/
Disallow: /drafts/

# Rules for Google's image bot
User-agent: Googlebot-Image
Allow: /projects/
Allow: /public/projects/
Disallow: /private-images/

# Rules for mobile-specific crawlers
User-agent: Googlebot-Mobile
Allow: /

# Rules for Bing's crawler
User-agent: Bingbot
Allow: /
Disallow: /api/
Disallow: /private/
Disallow: /admin/
Crawl-delay: 1

# Site maps - Next.js automatically serves the sitemap.xml generated from app/sitemap.ts
Sitemap: https://ahmed-abd-elmohsen.vercel.app/sitemap.xml

# Host directive helps specify the preferred domain version
Host: https://ahmed-abd-elmohsen.vercel.app